import hashlib
import json
import os
import tempfile
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

from tqdm import tqdm

from deepnotes.llm.llm_wrapper import get_llm_model
from deepnotes.loaders.document_loader import DocumentLoader
from deepnotes.models.analyzer_models import (
    ChunkAnalysisResult,
    ConsolidationAnalysisResult,
    Entity,
    KnowledgeGraph,
    Relationship,
)
from deepnotes.models.loader_models import (
    CodeFile,
    DocumentLoadedData,
    LoadedData,
)
from deepnotes.models.source_models import SourceConfig, SourceType
from deepnotes.storage.document_storage import DocumentStore


class ContentAnalyzer:
    def __init__(self, config: dict = None):
        """
        Initializes the first-layer document processor.
        """
        self.llm_model = get_llm_model()
        self.config = config or {}

        # Get chunking config with defaults
        chunk_config = self.config.get("text_processing", {})
        self.loader = DocumentLoader(
            SourceConfig(
                type=SourceType.DOCUMENT,
                name="document_loader",
                options={
                    "chunk_size": chunk_config.get("chunk_size", 1000),
                    "chunk_overlap": chunk_config.get("chunk_overlap", 100),
                },
            )
        )

        # Set default concurrency if not configured
        self.doc_concurrency = self.config.get(
            "document_processing", os.cpu_count() or 4
        )
        self.chunk_concurrency = self.config.get(
            "chunk_processing", os.cpu_count() or 4
        )

        self.document_store = DocumentStore(
            self.config.get("database_uri", "sqlite:///deepnotes.db")
        )

        self.document_cache = {}

    def process_loaded_data(
        self, data: LoadedData
    ) -> List[ConsolidationAnalysisResult]:
        """Route processing based on data type"""
        if data.data_type == "documents":
            doc_data = DocumentLoadedData(**data.model_dump())
            results = []
            for doc in tqdm(doc_data.documents, desc="Analyzing documents"):
                with tempfile.NamedTemporaryFile(mode="w", delete=False) as temp_file:
                    temp_file.write(doc.raw_content)
                    temp_file.flush()
                    result = self._process_document(temp_file.name)
                    if result:
                        result.source_metadata = doc.metadata.model_dump()
                        results.append(result)
                    os.unlink(temp_file.name)
            return results
        elif data.data_type == "codebase":
            return self._analyze_codebase(data.files, data.dependencies)
        elif data.data_type == "database":
            return self._analyze_database(data.tables)
        return []

    @staticmethod
    def _calculate_file_hash(file_path: str) -> str:
        """Calculate SHA-256 hash of file content"""
        sha256_hash = hashlib.sha256()
        with open(file_path, "rb") as f:
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()

    def _consolidate_results(
        self, chunk_analysis_results: List[ChunkAnalysisResult]
    ) -> ConsolidationAnalysisResult | None:
        """
        Process chunk-level intermediate data list for data consolidation and final output generation (comprehensive summary).
        Args:
            chunk_analysis_results (list): List of chunk-level intermediate data generated by first layer
        Returns:
            str: Final output (comprehensive document summary)
        """
        if not chunk_analysis_results:
            raise ValueError(
                "No chunk-level data provided for consolidation processing."
            )

        prompt = self._create_consolidation_prompt(chunk_analysis_results)

        # Initialize progress with chunk count
        total_chunks = len(chunk_analysis_results)
        with tqdm(
            total=total_chunks,
            desc=f"Consolidating {total_chunks} chunks",
            bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} chunks",
        ) as pbar:
            llm_response = self.llm_model.generate(
                prompt,
                response_model=ConsolidationAnalysisResult,
            )

            pbar.update(total_chunks)

        result = llm_response.model_instance
        return result

    @staticmethod
    def _create_chunk_analysis_prompt(chunk_content, chunk_index):
        """
        Construct chunk analysis prompt (example)
        """
        prompt_template = f"""Analyze the content of the following document chunk (chunk {
            chunk_index
        }) and extract the following information in JSON format:
        - Chunk index
        - Concise summary (under 500 words)
        - Core topics
        - Key entities

        Document chunk content:
        {chunk_content}

        Output JSON Schema:
        {ChunkAnalysisResult.model_json_schema()}

        Output JSON Example:
        {
            ChunkAnalysisResult(
                chunk_index=0,
                summary="This is a summary.",
                core_topics=["topic1", "topic2"],
                key_entities=["entity1", "entity2"],
            ).model_dump_json()
        }
        """
        return prompt_template

    @staticmethod
    def _create_consolidation_prompt(chunk_analysis_results: List[ChunkAnalysisResult]):
        """
        Construct document summary consolidation prompt (example)
        """
        prompt_template = f"""
        Based on the summaries from multiple document chunks below, generate a comprehensive document analysis including:
        - Overall summary
        - Knowledge graph structure
        - Metadata information

        Please be reminded:
        1. Return results in JSON format.
        2. Entity ID should be meaningful and unique, following snake case format. E.g. "knowledge_graph", "deep_learning".

        Document chunk information (in order):
        {[data.model_dump_json() for data in chunk_analysis_results]}

        Output JSON Schema:
        {ConsolidationAnalysisResult.model_json_schema()}

        Output JSON Example:
        {
            ConsolidationAnalysisResult(
                summary="This is a comprehensive document summary.",
                knowledge_graph=KnowledgeGraph(
                    entities=[
                        Entity(
                            id="knowledge_graph",
                            name="Knowledge Graph",
                            type="concept",
                        ),
                        Entity(
                            id="deep_learning",
                            name="Deep Learning",
                            type="concept",
                        ),
                    ],
                    relationships=[
                        Relationship(
                            source="knowledge_graph",
                            target="deep-learning",
                            type="depends_on",
                        ),
                    ],
                ),
            ).model_dump_json()
        }
        """
        return prompt_template

    def _analyze_chunk(self, chunk_content, chunk_index):
        """
        Analyze a single document chunk.
        """
        prompt = self._create_chunk_analysis_prompt(chunk_content, chunk_index)
        llm_response = self.llm_model.generate(
            prompt, response_model=ChunkAnalysisResult
        )

        analysis_result = ChunkAnalysisResult(
            **llm_response.model_instance.model_dump()
        )
        if analysis_result.chunk_index is None:
            analysis_result.chunk_index = chunk_index

        return analysis_result

    def _analyze_codebase(
        self, files: List[CodeFile], dependencies: List[str]
    ) -> List[ConsolidationAnalysisResult]:
        """Analyze codebase structure and relationships"""
        results = []
        for file in tqdm(files, desc="Analyzing code files"):
            if file.path.endswith(".py"):
                with tempfile.NamedTemporaryFile(mode="w", delete=False) as temp_file:
                    temp_file.write(file.content)
                    temp_file.flush()
                    result = self._process_document(temp_file.name)
                    if result:
                        result.source_metadata = file.metadata.model_dump()
                        result.content_type = "code"
                        results.append(result)
                    os.unlink(temp_file.name)
        return results

    def _analyze_database(
        self, tables: List[Dict]
    ) -> List[ConsolidationAnalysisResult]:
        """Analyze database schema and data patterns"""
        results = []
        for table in tqdm(tables, desc="Analyzing database tables"):
            # Convert table schema and data to processable format
            table_content = f"""
            Table Name: {table["name"]}
            Schema: {json.dumps(table["schema"], indent=2)}
            Sample Data: {json.dumps(table["sample_data"], indent=2)}
            """
            with tempfile.NamedTemporaryFile(mode="w", delete=False) as temp_file:
                temp_file.write(table_content)
                temp_file.flush()
                result = self._process_document(temp_file.name)
                if result:
                    result.source_metadata = table["metadata"]
                    result.content_type = "database"
                    results.append(result)
                os.unlink(temp_file.name)
        return results

    def _process_document(self, document_path) -> Optional[ConsolidationAnalysisResult]:
        """
        Process a single long document file.
        Args:
            document_path (str): Path to the document file
        Returns:
            list[dict]: List of chunk-level intermediate data, each element corresponds to a chunk's analysis result
        """
        # Calculate document hash
        content_hash = self._calculate_file_hash(document_path)

        # Check if document already processed
        existing_doc = self.document_store.get_document_by_path(document_path)
        if existing_doc and existing_doc.hash == content_hash:
            # Document unchanged, return cached analysis
            latest_analysis = existing_doc.analysis_results[-1]
            return ConsolidationAnalysisResult(**latest_analysis.analysis_data)

        # Process new document
        loaded_data = self.loader.process(target_path=document_path)
        document = self.document_store.store_document(
            document_path,
            content_hash,
            metadata={
                "filename": Path(document_path).name,
                "processed_at": datetime.utcnow().isoformat(),
            },
        )

        # Store chunks
        chunks = self.document_store.store_chunks(document.id, loaded_data.raw_data)

        # Process chunks and store analysis
        analysis_results = []
        for chunk in chunks:
            analysis = self._analyze_chunk(chunk.content, chunk.index)
            if analysis:
                self.document_store.store_chunk_analysis(
                    chunk.id, analysis.model_dump()
                )
                analysis_results.append(analysis)

        # Consolidate results
        consolidated = self._consolidate_results(analysis_results)
        if consolidated:
            self.document_store.store_analysis(document.id, consolidated.model_dump())

        return consolidated
